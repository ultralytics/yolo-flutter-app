// Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

package com.ultralytics.yolo

import android.content.Context
import android.graphics.*
import android.util.Log
import android.util.Size
import org.tensorflow.lite.DataType
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.gpu.GpuDelegate
import org.tensorflow.lite.support.common.FileUtil
import org.tensorflow.lite.support.common.ops.CastOp
import org.tensorflow.lite.support.common.ops.NormalizeOp
import org.tensorflow.lite.support.image.ImageProcessor
import org.tensorflow.lite.support.image.TensorImage
import org.tensorflow.lite.support.image.ops.ResizeOp
import org.tensorflow.lite.support.image.ops.Rot90Op
import org.tensorflow.lite.support.metadata.MetadataExtractor
import org.tensorflow.lite.support.metadata.schema.ModelMetadata
import org.yaml.snakeyaml.Yaml
import kotlin.math.max
import kotlin.math.min

class PoseEstimator(
    context: Context,
    modelPath: String,
    override var labels: List<String>,
    private val useGpu: Boolean = true,
    private val confidenceThreshold: Float = 0.25f,   // ä»»æ„ã§å¤‰æ›´
    private val iouThreshold: Float = 0.45f           // ä»»æ„ã§å¤‰æ›´
) : BasePredictor() {

    companion object {
        // xywh(4) + conf(1) + keypoints(17*3=51) = 56
        private const val OUTPUT_FEATURES = 56
        private const val KEYPOINTS_COUNT = 17
        private const val KEYPOINTS_FEATURES = KEYPOINTS_COUNT * 3 // x, y, conf per keypoint
    }

    private val interpreterOptions = Interpreter.Options().apply {
        if (useGpu) {
            try {
                addDelegate(GpuDelegate())
            } catch (e: Exception) {
                Log.e("PoseEstimator", "GPU delegate error: ${e.message}")
            }
        }
    }

    private lateinit var imageProcessorCamera: ImageProcessor
    private lateinit var imageProcessorSingleImage: ImageProcessor

    init {
        // (1) TFLiteãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (æ‹¡å¼µå­è‡ªå‹•ä»˜ä¸)
        val modelBuffer = YoloUtils.loadModelFile(context, modelPath)

        // ===== ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰ =====
        try {
            val metadataExtractor = MetadataExtractor(modelBuffer)
            val modelMetadata: ModelMetadata? = metadataExtractor.modelMetadata
            if (modelMetadata != null) {
                Log.d("PoseEstimator", "Model metadata retrieved successfully.")
            }
            val associatedFiles = metadataExtractor.associatedFileNames
            if (!associatedFiles.isNullOrEmpty()) {
                for (fileName in associatedFiles) {
                    Log.d("PoseEstimator", "Found associated file: $fileName")
                    metadataExtractor.getAssociatedFile(fileName)?.use { stream ->
                        val fileContent = stream.readBytes()
                        val fileString = fileContent.toString(Charsets.UTF_8)
                        try {
                            val yaml = Yaml()
                            @Suppress("UNCHECKED_CAST")
                            val data = yaml.load<Map<String, Any>>(fileString)
                            if (data != null && data.containsKey("names")) {
                                val namesMap = data["names"] as? Map<Int, String>
                                if (namesMap != null) {
                                    this.labels = namesMap.values.toList()
                                    Log.d("PoseEstimator", "Loaded labels from metadata: $labels")
                                } else {

                                }
                            } else {

                            }
                        } catch (ex: Exception) {
                            Log.e("PoseEstimator", "Failed to parse YAML from metadata: ${ex.message}")
                        }
                    }
                }
            } else {
                Log.d("PoseEstimator", "No associated files found in the metadata.")
            }
        } catch (e: Exception) {
            Log.e("PoseEstimator", "Failed to extract metadata: ${e.message}")
        }

        // (2) Interpreterã®ç”Ÿæˆ
        interpreter = Interpreter(modelBuffer, interpreterOptions)

        // (3) å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã‚’å–å¾—: [1, height, width, 3]
        val inputShape = interpreter.getInputTensor(0).shape()
        val inHeight = inputShape[1]
        val inWidth = inputShape[2]
        inputSize = com.ultralytics.yolo.Size(inWidth, inHeight)
        modelInputSize = Pair(inWidth, inHeight)

        // (4) ImageProcessorã®åˆæœŸåŒ– - both with and without rotation
        
        // For camera feed (with rotation)
        imageProcessorCamera = ImageProcessor.Builder()
            .add(Rot90Op(3)) // å¿…è¦ã«å¿œã˜ã¦å›è»¢ã™ã‚‹å ´åˆã¯æ•°å€¤ã‚’å¤‰ãˆã¦ãã ã•ã„
            .add(ResizeOp(inHeight, inWidth, ResizeOp.ResizeMethod.BILINEAR))
            .add(NormalizeOp(0f, 255f))
            .add(CastOp(DataType.FLOAT32))
            .build()
            
        // For single images (no rotation needed)
        imageProcessorSingleImage = ImageProcessor.Builder()
            .add(ResizeOp(inHeight, inWidth, ResizeOp.ResizeMethod.BILINEAR))
            .add(NormalizeOp(0f, 255f))
            .add(CastOp(DataType.FLOAT32))
            .build()
    }

    override fun predict(bitmap: Bitmap, origWidth: Int, origHeight: Int, rotateForCamera: Boolean): YOLOResult {
        t0 = System.nanoTime()
        // (1) å‰å‡¦ç†: TensorImageã¸ãƒ­ãƒ¼ãƒ‰ & ImageProcessorã§å‡¦ç†
        val tensorImage = TensorImage(DataType.FLOAT32)
        tensorImage.load(bitmap)
        
        // Choose the appropriate processor based on input source
        val processedImage = if (rotateForCamera) {
            // Apply rotation for camera feed
            imageProcessorCamera.process(tensorImage)
        } else {
            // No rotation for single image
            imageProcessorSingleImage.process(tensorImage)
        }
        val inputBuffer = processedImage.buffer

        // (2) å‡ºåŠ›ç”¨é…åˆ— [1, 56, N] (N=2100ç­‰)
        val outputShape = interpreter.getOutputTensor(0).shape()  // ä¾‹: [1, 56, 2100]
        val batchSize = outputShape[0]           // 1
        val outFeatures = outputShape[1]         // 56
        val numAnchors = outputShape[2]          // 2100ç­‰
        require(outFeatures == OUTPUT_FEATURES) {
            "Unexpected output feature size. Expected=$OUTPUT_FEATURES, Actual=$outFeatures"
        }

        // å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ä½œæˆ
        val outputArray = Array(batchSize) {
            Array(outFeatures) { FloatArray(numAnchors) }
        }

        // (3) æ¨è«–å®Ÿè¡Œ
        interpreter.run(inputBuffer, outputArray)
        // å‡¦ç†æ™‚é–“ã®è¨ˆæ¸¬æ›´æ–°
        updateTiming()

        // (4) å¾Œå‡¦ç†: NMS + ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆè¨ˆç®—
        val rawDetections = postProcessPose(
            features = outputArray[0],  // shape: [56][numAnchors]
            numAnchors = numAnchors,
            confidenceThreshold = confidenceThreshold,
            iouThreshold = iouThreshold,
            origWidth = origWidth,
            origHeight = origHeight
        )

        // æ¤œå‡ºçµæœã‚’å–ã‚Šå‡ºã—
        val boxes = rawDetections.map { it.box }
        val keypointsList = rawDetections.map { it.keypoints }

        // ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ãƒ“ãƒƒãƒˆãƒãƒƒãƒ—ç”Ÿæˆ
//        val annotatedImage = drawPoseOnBitmap(bitmap, keypointsList, boxes)

        val fpsDouble: Double = if (t4 > 0) (1.0 / t4) else 0.0
        // YOLOResultã«è©°ã‚ã¦è¿”ã™
        return YOLOResult(
            origShape = com.ultralytics.yolo.Size(bitmap.height, bitmap.width),
            boxes = boxes,
            keypointsList = keypointsList,
//            annotatedImage = annotatedImage,
            speed = t2,   // ãƒŸãƒªç§’ç­‰ã®æ¸¬å®šå€¤ã¯BasePredictorå´ã®å®Ÿè£…æ¬¡ç¬¬
            fps = fpsDouble,
            names = labels
        )
    }

    /**
     * å¾Œå‡¦ç†: Confidenceé–¾å€¤ã§é™¤å¤–ã€NMSã§æŠ‘åˆ¶ã€åº§æ¨™å¤‰æ›ã€Keypointså‡¦ç†
     */
    private fun postProcessPose(
        features: Array<FloatArray>,
        numAnchors: Int,
        confidenceThreshold: Float,
        iouThreshold: Float,
        origWidth: Int,
        origHeight: Int
    ): List<PoseDetection> {

        val detections = mutableListOf<PoseDetection>()

        // ä¾‹ãˆã° modelInputSize = (640, 640) ã¨ä»®å®š
        val (modelW, modelH) = modelInputSize
        val scaleX = origWidth.toFloat() / modelW
        val scaleY = origHeight.toFloat() / modelH

        for (j in 0 until numAnchors) {
            // ä¾‹: features[0][j] ~ features[3][j] ã¯ 0ï½1 ã®æ­£è¦åŒ–å€¤
            val rawX = features[0][j]       // 0..1
            val rawY = features[1][j]       // 0..1
            val rawW = features[2][j]       // 0..1
            val rawH = features[3][j]       // 0..1
            val conf = features[4][j]       // 0..1

            if (conf < confidenceThreshold) continue

            // (A) ã¾ãšæ­£è¦åŒ–ã‚’ãƒ¢ãƒ‡ãƒ«å…¥åŠ›è§£åƒåº¦ã«æ‹¡å¤§ (640ç­‰) ã™ã‚‹
            val xScaled = rawX * modelW
            val yScaled = rawY * modelH
            val wScaled = rawW * modelW
            val hScaled = rawH * modelH

            // [x-w/2, y-h/2, x+w/2, y+h/2] ã¯ modelInputSize ã‚¹ã‚±ãƒ¼ãƒ«
            val left   = xScaled - wScaled / 2f
            val top    = yScaled - hScaled / 2f
            val right  = xScaled + wScaled / 2f
            val bottom = yScaled + hScaled / 2f

            // modelInputSize ã‚¹ã‚±ãƒ¼ãƒ«ã§ã® RectF
            val normBox = RectF(left / modelW, top / modelH, right / modelW, bottom / modelH)

            // (B) ä»Šåº¦ã¯å®Ÿéš›ã®å…ƒç”»åƒã‚¹ã‚±ãƒ¼ãƒ«ã¸æ‹¡å¤§
            val rectF = RectF(
                left   * scaleX,
                top    * scaleY,
                right  * scaleX,
                bottom * scaleY
            )

            // ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆ (5..55) ã‚‚åŒæ§˜ã«ãƒ¢ãƒ‡ãƒ«è§£åƒåº¦ â†’ å®Ÿç”»åƒã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›
            val kpArray = mutableListOf<Pair<Float, Float>>()
            val kpConfArray = mutableListOf<Float>()
            for (k in 0 until KEYPOINTS_COUNT) {
                val rawKx = features[5 + k * 3][j] // 0..1
                val rawKy = features[5 + k * 3 + 1][j]
                val kpC   = features[5 + k * 3 + 2][j]

                // ãƒ¢ãƒ‡ãƒ«å…¥åŠ›è§£åƒåº¦ã‚¹ã‚±ãƒ¼ãƒ«ã¸
                val kxScaled = rawKx * modelW
                val kyScaled = rawKy * modelH

                // å…ƒç”»åƒã‚¹ã‚±ãƒ¼ãƒ«ã¸
                val finalKx = kxScaled * scaleX
                val finalKy = kyScaled * scaleY

                kpArray.add(finalKx to finalKy)
                kpConfArray.add(kpC)
            }

            val keypointsObj = Keypoints(
                xyn = kpArray.map { (fx, fy) ->
                    // 0ï½1 ã«å†æ­£è¦åŒ–ã™ã‚‹å ´åˆ: (fx / origWidth, fy / origHeight)
                    (fx / origWidth) to (fy / origHeight)
                },
                xy = kpArray,      // å®Ÿç”»åƒåº§æ¨™
                conf = kpConfArray
            )

            detections.add(
                PoseDetection(
                    box = Box(index = 0, cls = "person", conf = conf, xywh = rectF, xywhn = normBox),
                    keypoints = keypointsObj
                )
            )
        }

        // ä»¥é™ã¯ NMS å‡¦ç† (å˜ä¸€ã‚¯ãƒ©ã‚¹æƒ³å®š) ã¯å¤‰ã‚ã‚‰ãš
        val finalDetections = nmsPoseDetections(detections, iouThreshold)
        return finalDetections
    }


    /**
     * NMSã‚’å˜ä¸€ã‚¯ãƒ©ã‚¹æƒ³å®šã§å®Ÿè¡Œ (æ‹¡å¼µã—ãŸã„å ´åˆã¯ã‚¯ãƒ©ã‚¹åˆ¥ã«åˆ†å‰²ã—ã¦NMS)
     */
    private fun nmsPoseDetections(
        detections: List<PoseDetection>,
        iouThreshold: Float
    ): List<PoseDetection> {
        // ä¿¡é ¼åº¦ã§é™é †ã‚½ãƒ¼ãƒˆ
        val sorted = detections.sortedByDescending { it.box.conf }
        val picked = mutableListOf<PoseDetection>()
        val used = BooleanArray(sorted.size)

        for (i in sorted.indices) {
            if (used[i]) continue

            val d1 = sorted[i]
            picked.add(d1)

            for (j in i + 1 until sorted.size) {
                if (used[j]) continue
                val d2 = sorted[j]
                if (iou(d1.box.xywh, d2.box.xywh) > iouThreshold) {
                    used[j] = true
                }
            }
        }
        return picked
    }

    /**
     * IoUè¨ˆç®—
     */
    private fun iou(a: RectF, b: RectF): Float {
        val interLeft = max(a.left, b.left)
        val interTop = max(a.top, b.top)
        val interRight = min(a.right, b.right)
        val interBottom = min(a.bottom, b.bottom)
        val interW = max(0f, interRight - interLeft)
        val interH = max(0f, interBottom - interTop)
        val interArea = interW * interH
        val unionArea = a.width() * a.height() + b.width() * b.height() - interArea
        return if (unionArea <= 0f) 0f else (interArea / unionArea)
    }

    /**
     * ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’æç”»ã—ãŸãƒ“ãƒƒãƒˆãƒãƒƒãƒ—ã‚’è¿”ã™
     */
    private fun drawPoseOnBitmap(
        bitmap: Bitmap,
        keypointsList: List<Keypoints>,
        boxes: List<Box>
    ): Bitmap {
        val output = bitmap.copy(Bitmap.Config.ARGB_8888, true)
        val canvas = Canvas(output)
        val paint = Paint().apply {
            style = Paint.Style.FILL
            color = Color.GREEN
            strokeWidth = 5f
        }
        // ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»ç”¨
        val boxPaint = Paint().apply {
            style = Paint.Style.STROKE
            color = Color.RED
            strokeWidth = 3f
        }

        // å„Personã«ã¤ã„ã¦æç”»
        for ((index, person) in keypointsList.withIndex()) {
            // ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã®æç”»
            val boxRect = boxes[index].xywh
            canvas.drawRect(boxRect, boxPaint)

            // ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®æç”»
            for ((i, kp) in person.xy.withIndex()) {
                // confãŒä¸€å®šä»¥ä¸Šã®ã¿å¯è¦–åŒ–ã—ãŸã‘ã‚Œã°é©å®œåˆ¤å®š
                if (person.conf[i] > 0.25f) {
                    canvas.drawCircle(kp.first, kp.second, 8f, paint)
                }
            }
            // å¿…è¦ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ«ãƒˆãƒ³(éª¨æ ¼ç·š)æç”»ã‚’è¿½åŠ 
        }
        return output
    }

    /**
     * PoseDetection ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
     */
    private data class PoseDetection(
        val box: Box,
        val keypoints: Keypoints
    )
}
